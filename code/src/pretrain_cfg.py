max_steps = 5000
warmup_steps = 0.1
warmup_rate=0.1
swa_start = -1
ema_start = -1
ema_decay = 0.99
distributed_train = False
fgm=0
pgd=0
best_loss=10000.
max_epochs = 10
print_steps = 200
save_steps = 200
bert_seq_length=128
batch_size = 32
val_batch_size = 64
copy_time = 2
seed=42
negative_rate=0.5
max_grad_norm=1.
learning_rate=4e-5
bert_learning_rate = 4e-5
weight_decay=0.01
adam_epsilon=1e-6
premise = False
NSP = True
gpu_ids="0"
savedmodel_path="data/pretrain_mlm_nsp_tweet_v2/"
ckpt_file=None
bert_dir="digitalepidemiologylab/covid-twitter-bert-v2"
# bert_dir="roberta-large"

bert_cache="data/bert_cache"
train_path=["data/train.tsv"]
valid_path=["data/validation.tsv"]